{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = \"cpu\" #\"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetTransforms():\n",
    "    '''Returns a list of transformations when type as requested amongst train/test\n",
    "       Transforms('train') = list of transforms to apply on training data\n",
    "       Transforms('test') = list of transforms to apply on testing data'''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def trainparams(self):\n",
    "        train_transformations = [ #resises the image so it can be perfect for our model.\n",
    "            transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
    "            transforms.RandomRotation((-7,7)),     #Rotates the image to a specified angel\n",
    "            transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n",
    "            transforms.ToTensor(), # comvert the image to tensor so that it can work with torch\n",
    "            transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261)) #Normalize all the images\n",
    "            ]\n",
    "\n",
    "        return train_transformations\n",
    "\n",
    "    def testparams(self):\n",
    "        test_transforms = [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261))\n",
    "        ]\n",
    "        return test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "transformations = GetTransforms()\n",
    "train_transforms = transforms.Compose(transformations.trainparams())\n",
    "test_transforms = transforms.Compose(transformations.testparams())\n",
    "\n",
    "\n",
    "class GetCIFAR10_TrainData():\n",
    "    def __init__(self, dir_name:str):\n",
    "        self.dirname = dir_name\n",
    "\n",
    "    def download_train_data(self):\n",
    "        return datasets.CIFAR10('./data', train=True, download=True, transform=train_transforms)\n",
    "\n",
    "    def download_test_data(self):\n",
    "        return datasets.CIFAR10('./data', train=False, download=True, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36af5edd2604cfca70226a468f929d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data = GetCIFAR10_TrainData(os.chdir(\"..\"))\n",
    "trainset = data.download_train_data()\n",
    "testset = data.download_test_data()\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=512,\n",
    "                                         shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        DROPOUT = 0.1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes),\n",
    "                nn.Dropout(DROPOUT))\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.dropout(self.bn1(self.conv1(x))))\n",
    "        out = self.dropout(self.bn2(self.conv2(out)))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "           Dropout-5           [-1, 64, 32, 32]               0\n",
      "            Conv2d-6           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
      "           Dropout-8           [-1, 64, 32, 32]               0\n",
      "        BasicBlock-9           [-1, 64, 32, 32]               0\n",
      "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "          Dropout-12           [-1, 64, 32, 32]               0\n",
      "           Conv2d-13           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-14           [-1, 64, 32, 32]             128\n",
      "          Dropout-15           [-1, 64, 32, 32]               0\n",
      "       BasicBlock-16           [-1, 64, 32, 32]               0\n",
      "           Conv2d-17          [-1, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-18          [-1, 128, 16, 16]             256\n",
      "          Dropout-19          [-1, 128, 16, 16]               0\n",
      "           Conv2d-20          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-21          [-1, 128, 16, 16]             256\n",
      "          Dropout-22          [-1, 128, 16, 16]               0\n",
      "           Conv2d-23          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-24          [-1, 128, 16, 16]             256\n",
      "          Dropout-25          [-1, 128, 16, 16]               0\n",
      "       BasicBlock-26          [-1, 128, 16, 16]               0\n",
      "           Conv2d-27          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-28          [-1, 128, 16, 16]             256\n",
      "          Dropout-29          [-1, 128, 16, 16]               0\n",
      "           Conv2d-30          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-31          [-1, 128, 16, 16]             256\n",
      "          Dropout-32          [-1, 128, 16, 16]               0\n",
      "       BasicBlock-33          [-1, 128, 16, 16]               0\n",
      "           Conv2d-34            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-35            [-1, 256, 8, 8]             512\n",
      "          Dropout-36            [-1, 256, 8, 8]               0\n",
      "           Conv2d-37            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-38            [-1, 256, 8, 8]             512\n",
      "          Dropout-39            [-1, 256, 8, 8]               0\n",
      "           Conv2d-40            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 8, 8]             512\n",
      "          Dropout-42            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-43            [-1, 256, 8, 8]               0\n",
      "           Conv2d-44            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 8, 8]             512\n",
      "          Dropout-46            [-1, 256, 8, 8]               0\n",
      "           Conv2d-47            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 8, 8]             512\n",
      "          Dropout-49            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-50            [-1, 256, 8, 8]               0\n",
      "           Conv2d-51            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 4, 4]           1,024\n",
      "          Dropout-53            [-1, 512, 4, 4]               0\n",
      "           Conv2d-54            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 4, 4]           1,024\n",
      "          Dropout-56            [-1, 512, 4, 4]               0\n",
      "           Conv2d-57            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-58            [-1, 512, 4, 4]           1,024\n",
      "          Dropout-59            [-1, 512, 4, 4]               0\n",
      "       BasicBlock-60            [-1, 512, 4, 4]               0\n",
      "           Conv2d-61            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-62            [-1, 512, 4, 4]           1,024\n",
      "          Dropout-63            [-1, 512, 4, 4]               0\n",
      "           Conv2d-64            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-65            [-1, 512, 4, 4]           1,024\n",
      "          Dropout-66            [-1, 512, 4, 4]               0\n",
      "       BasicBlock-67            [-1, 512, 4, 4]               0\n",
      "           Linear-68                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,173,962\n",
      "Trainable params: 11,173,962\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 15.44\n",
      "Params size (MB): 42.63\n",
      "Estimated Total Size (MB): 58.07\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnemonic/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "model = ResNet18().to(device)\n",
    "summary(model, input_size=(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model, device, train_dataloader, optimizer, train_acc, train_losses):\n",
    "            \n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data)\n",
    "        loss = F.nll_loss(y_pred, target)        \n",
    "\n",
    "        train_losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = y_pred.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        processed += len(data)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_description(desc=f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
    "        train_acc.append(100*correct/processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_testing(model, device, test_dataloader, test_acc, test_losses, misclassified = []):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    # label = 0\n",
    "    classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for index, (data, target) in enumerate(test_dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            for d,i,j in zip(data, pred, target):\n",
    "                if i != j:\n",
    "                    misclassified.append([d.cpu(),i[0].cpu(),j.cpu()])\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_dataloader.dataset),\n",
    "        100. * correct / len(test_dataloader.dataset)))\n",
    "    \n",
    "    test_acc.append(100. * correct / len(test_dataloader.dataset))\n",
    "    return misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/98 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss=2.42653226852417 Batch_id=0 Accuracy=8.98:   0%|          | 0/98 [00:27<?, ?it/s]\u001b[A\n",
      "Loss=2.42653226852417 Batch_id=0 Accuracy=8.98:   1%|          | 1/98 [00:27<44:42, 27.65s/it]\u001b[A\n",
      "Loss=2.310129165649414 Batch_id=1 Accuracy=9.38:   1%|          | 1/98 [00:55<44:42, 27.65s/it]\u001b[A\n",
      "Loss=2.310129165649414 Batch_id=1 Accuracy=9.38:   2%|▏         | 2/98 [00:55<44:25, 27.76s/it]\u001b[A\n",
      "Loss=2.2783706188201904 Batch_id=2 Accuracy=10.94:   2%|▏         | 2/98 [01:24<44:25, 27.76s/it]\u001b[A\n",
      "Loss=2.2783706188201904 Batch_id=2 Accuracy=10.94:   3%|▎         | 3/98 [01:24<44:35, 28.16s/it]\u001b[A\n",
      "Loss=2.2912161350250244 Batch_id=3 Accuracy=11.82:   3%|▎         | 3/98 [01:53<44:35, 28.16s/it]\u001b[A\n",
      "Loss=2.2912161350250244 Batch_id=3 Accuracy=11.82:   4%|▍         | 4/98 [01:53<44:28, 28.39s/it]\u001b[A\n",
      "Loss=2.2706801891326904 Batch_id=4 Accuracy=11.91:   4%|▍         | 4/98 [02:23<44:28, 28.39s/it]\u001b[A\n",
      "Loss=2.2706801891326904 Batch_id=4 Accuracy=11.91:   5%|▌         | 5/98 [02:23<44:29, 28.71s/it]\u001b[A\n",
      "Loss=2.2741122245788574 Batch_id=5 Accuracy=12.14:   5%|▌         | 5/98 [02:50<44:29, 28.71s/it]\u001b[A\n",
      "Loss=2.2741122245788574 Batch_id=5 Accuracy=12.14:   6%|▌         | 6/98 [02:50<43:24, 28.31s/it]\u001b[A\n",
      "Loss=2.2784242630004883 Batch_id=6 Accuracy=12.44:   6%|▌         | 6/98 [03:21<43:24, 28.31s/it]\u001b[A\n",
      "Loss=2.2784242630004883 Batch_id=6 Accuracy=12.44:   7%|▋         | 7/98 [03:21<44:04, 29.06s/it]\u001b[A\n",
      "Loss=2.244141101837158 Batch_id=7 Accuracy=12.84:   7%|▋         | 7/98 [03:49<44:04, 29.06s/it] \u001b[A\n",
      "Loss=2.244141101837158 Batch_id=7 Accuracy=12.84:   8%|▊         | 8/98 [03:49<43:02, 28.70s/it]\u001b[A\n",
      "Loss=2.2412047386169434 Batch_id=8 Accuracy=12.93:   8%|▊         | 8/98 [04:16<43:02, 28.70s/it]\u001b[A\n",
      "Loss=2.2412047386169434 Batch_id=8 Accuracy=12.93:   9%|▉         | 9/98 [04:16<41:46, 28.16s/it]\u001b[A\n",
      "Loss=2.135021209716797 Batch_id=9 Accuracy=13.55:   9%|▉         | 9/98 [04:40<41:46, 28.16s/it] \u001b[A\n",
      "Loss=2.135021209716797 Batch_id=9 Accuracy=13.55:  10%|█         | 10/98 [04:40<39:50, 27.17s/it]\u001b[A\n",
      "Loss=2.0851869583129883 Batch_id=10 Accuracy=14.67:  10%|█         | 10/98 [05:09<39:50, 27.17s/it]\u001b[A\n",
      "Loss=2.0851869583129883 Batch_id=10 Accuracy=14.67:  11%|█         | 11/98 [05:09<40:07, 27.68s/it]\u001b[A\n",
      "Loss=2.086097478866577 Batch_id=11 Accuracy=15.25:  11%|█         | 11/98 [05:36<40:07, 27.68s/it] \u001b[A\n",
      "Loss=2.086097478866577 Batch_id=11 Accuracy=15.25:  12%|█▏        | 12/98 [05:36<39:13, 27.37s/it]\u001b[A\n",
      "Loss=2.1195151805877686 Batch_id=12 Accuracy=15.66:  12%|█▏        | 12/98 [06:03<39:13, 27.37s/it]\u001b[A\n",
      "Loss=2.1195151805877686 Batch_id=12 Accuracy=15.66:  13%|█▎        | 13/98 [06:03<38:31, 27.20s/it]\u001b[A\n",
      "Loss=2.016993999481201 Batch_id=13 Accuracy=16.32:  13%|█▎        | 13/98 [06:29<38:31, 27.20s/it] \u001b[A\n",
      "Loss=2.016993999481201 Batch_id=13 Accuracy=16.32:  14%|█▍        | 14/98 [06:29<37:32, 26.81s/it]\u001b[A\n",
      "Loss=2.0015690326690674 Batch_id=14 Accuracy=16.81:  14%|█▍        | 14/98 [06:56<37:32, 26.81s/it]\u001b[A\n",
      "Loss=2.0015690326690674 Batch_id=14 Accuracy=16.81:  15%|█▌        | 15/98 [06:56<37:19, 26.98s/it]\u001b[A\n",
      "Loss=2.0323729515075684 Batch_id=15 Accuracy=17.20:  15%|█▌        | 15/98 [07:25<37:19, 26.98s/it]\u001b[A\n",
      "Loss=2.0323729515075684 Batch_id=15 Accuracy=17.20:  16%|█▋        | 16/98 [07:25<37:34, 27.49s/it]\u001b[A\n",
      "Loss=2.0145866870880127 Batch_id=16 Accuracy=17.51:  16%|█▋        | 16/98 [07:55<37:34, 27.49s/it]\u001b[A\n",
      "Loss=2.0145866870880127 Batch_id=16 Accuracy=17.51:  17%|█▋        | 17/98 [07:55<38:07, 28.24s/it]\u001b[A\n",
      "Loss=2.0127334594726562 Batch_id=17 Accuracy=17.88:  17%|█▋        | 17/98 [08:25<38:07, 28.24s/it]\u001b[A\n",
      "Loss=2.0127334594726562 Batch_id=17 Accuracy=17.88:  18%|█▊        | 18/98 [08:25<38:39, 28.99s/it]\u001b[A\n",
      "Loss=1.9406261444091797 Batch_id=18 Accuracy=18.46:  18%|█▊        | 18/98 [08:52<38:39, 28.99s/it]\u001b[A\n",
      "Loss=1.9406261444091797 Batch_id=18 Accuracy=18.46:  19%|█▉        | 19/98 [08:52<37:20, 28.36s/it]\u001b[A\n",
      "Loss=2.0197360515594482 Batch_id=19 Accuracy=18.76:  19%|█▉        | 19/98 [09:18<37:20, 28.36s/it]\u001b[A\n",
      "Loss=2.0197360515594482 Batch_id=19 Accuracy=18.76:  20%|██        | 20/98 [09:18<35:51, 27.59s/it]\u001b[A\n",
      "Loss=1.951176404953003 Batch_id=20 Accuracy=19.17:  20%|██        | 20/98 [09:43<35:51, 27.59s/it] \u001b[A\n",
      "Loss=1.951176404953003 Batch_id=20 Accuracy=19.17:  21%|██▏       | 21/98 [09:43<34:27, 26.85s/it]\u001b[A\n",
      "Loss=1.8283270597457886 Batch_id=21 Accuracy=19.79:  21%|██▏       | 21/98 [10:08<34:27, 26.85s/it]\u001b[A\n",
      "Loss=1.8283270597457886 Batch_id=21 Accuracy=19.79:  22%|██▏       | 22/98 [10:08<33:02, 26.08s/it]\u001b[A\n",
      "Loss=1.823773741722107 Batch_id=22 Accuracy=20.35:  22%|██▏       | 22/98 [10:31<33:02, 26.08s/it] \u001b[A\n",
      "Loss=1.823773741722107 Batch_id=22 Accuracy=20.35:  23%|██▎       | 23/98 [10:31<31:47, 25.43s/it]\u001b[A\n",
      "Loss=1.8624417781829834 Batch_id=23 Accuracy=20.61:  23%|██▎       | 23/98 [10:56<31:47, 25.43s/it]\u001b[A\n",
      "Loss=1.8624417781829834 Batch_id=23 Accuracy=20.61:  24%|██▍       | 24/98 [10:56<31:07, 25.23s/it]\u001b[A\n",
      "Loss=1.8527271747589111 Batch_id=24 Accuracy=21.09:  24%|██▍       | 24/98 [11:21<31:07, 25.23s/it]\u001b[A\n",
      "Loss=1.8527271747589111 Batch_id=24 Accuracy=21.09:  26%|██▌       | 25/98 [11:21<30:22, 24.97s/it]\u001b[A\n",
      "Loss=1.7997422218322754 Batch_id=25 Accuracy=21.54:  26%|██▌       | 25/98 [11:44<30:22, 24.97s/it]\u001b[A\n",
      "Loss=1.7997422218322754 Batch_id=25 Accuracy=21.54:  27%|██▋       | 26/98 [11:44<29:33, 24.63s/it]\u001b[A\n",
      "Loss=1.8600789308547974 Batch_id=26 Accuracy=21.80:  27%|██▋       | 26/98 [12:08<29:33, 24.63s/it]\u001b[A\n",
      "Loss=1.8600789308547974 Batch_id=26 Accuracy=21.80:  28%|██▊       | 27/98 [12:08<28:48, 24.34s/it]\u001b[A\n",
      "Loss=1.9004380702972412 Batch_id=27 Accuracy=22.12:  28%|██▊       | 27/98 [12:32<28:48, 24.34s/it]\u001b[A\n",
      "Loss=1.9004380702972412 Batch_id=27 Accuracy=22.12:  29%|██▊       | 28/98 [12:32<28:11, 24.16s/it]\u001b[A\n",
      "Loss=1.7764374017715454 Batch_id=28 Accuracy=22.47:  29%|██▊       | 28/98 [13:00<28:11, 24.16s/it]\u001b[A\n",
      "Loss=1.7764374017715454 Batch_id=28 Accuracy=22.47:  30%|██▉       | 29/98 [13:00<29:18, 25.49s/it]\u001b[A\n",
      "Loss=1.780387043952942 Batch_id=29 Accuracy=22.81:  30%|██▉       | 29/98 [13:26<29:18, 25.49s/it] \u001b[A\n",
      "Loss=1.780387043952942 Batch_id=29 Accuracy=22.81:  31%|███       | 30/98 [13:26<29:04, 25.66s/it]\u001b[A\n",
      "Loss=1.7839429378509521 Batch_id=30 Accuracy=23.14:  31%|███       | 30/98 [13:57<29:04, 25.66s/it]\u001b[A\n",
      "Loss=1.7839429378509521 Batch_id=30 Accuracy=23.14:  32%|███▏      | 31/98 [13:57<30:08, 26.99s/it]\u001b[A\n",
      "Loss=1.7787772417068481 Batch_id=31 Accuracy=23.41:  32%|███▏      | 31/98 [14:26<30:08, 26.99s/it]\u001b[A\n",
      "Loss=1.7787772417068481 Batch_id=31 Accuracy=23.41:  33%|███▎      | 32/98 [14:26<30:23, 27.63s/it]\u001b[A\n",
      "Loss=1.756659984588623 Batch_id=32 Accuracy=23.73:  33%|███▎      | 32/98 [14:52<30:23, 27.63s/it] \u001b[A\n",
      "Loss=1.756659984588623 Batch_id=32 Accuracy=23.73:  34%|███▎      | 33/98 [14:52<29:25, 27.17s/it]\u001b[A\n",
      "Loss=1.7895805835723877 Batch_id=33 Accuracy=23.99:  34%|███▎      | 33/98 [15:19<29:25, 27.17s/it]\u001b[A\n",
      "Loss=1.7895805835723877 Batch_id=33 Accuracy=23.99:  35%|███▍      | 34/98 [15:19<29:01, 27.21s/it]\u001b[A\n",
      "Loss=1.7307721376419067 Batch_id=34 Accuracy=24.24:  35%|███▍      | 34/98 [15:44<29:01, 27.21s/it]\u001b[A\n",
      "Loss=1.7307721376419067 Batch_id=34 Accuracy=24.24:  36%|███▌      | 35/98 [15:44<27:42, 26.38s/it]\u001b[A\n",
      "Loss=1.71010160446167 Batch_id=35 Accuracy=24.54:  36%|███▌      | 35/98 [16:09<27:42, 26.38s/it]  \u001b[A\n",
      "Loss=1.71010160446167 Batch_id=35 Accuracy=24.54:  37%|███▋      | 36/98 [16:09<26:58, 26.11s/it]\u001b[A\n",
      "Loss=1.762263536453247 Batch_id=36 Accuracy=24.80:  37%|███▋      | 36/98 [16:35<26:58, 26.11s/it]\u001b[A\n",
      "Loss=1.762263536453247 Batch_id=36 Accuracy=24.80:  38%|███▊      | 37/98 [16:35<26:27, 26.02s/it]\u001b[A\n",
      "Loss=1.645931363105774 Batch_id=37 Accuracy=25.15:  38%|███▊      | 37/98 [17:05<26:27, 26.02s/it]\u001b[A\n",
      "Loss=1.645931363105774 Batch_id=37 Accuracy=25.15:  39%|███▉      | 38/98 [17:05<27:12, 27.21s/it]\u001b[A\n",
      "Loss=1.7284249067306519 Batch_id=38 Accuracy=25.35:  39%|███▉      | 38/98 [17:35<27:12, 27.21s/it]\u001b[A\n",
      "Loss=1.7284249067306519 Batch_id=38 Accuracy=25.35:  40%|███▉      | 39/98 [17:35<27:42, 28.18s/it]\u001b[A\n",
      "Loss=1.635400414466858 Batch_id=39 Accuracy=25.59:  40%|███▉      | 39/98 [18:01<27:42, 28.18s/it] \u001b[A\n",
      "Loss=1.635400414466858 Batch_id=39 Accuracy=25.59:  41%|████      | 40/98 [18:01<26:39, 27.58s/it]\u001b[A\n",
      "Loss=1.6401267051696777 Batch_id=40 Accuracy=25.82:  41%|████      | 40/98 [18:27<26:39, 27.58s/it]\u001b[A\n",
      "Loss=1.6401267051696777 Batch_id=40 Accuracy=25.82:  42%|████▏     | 41/98 [18:27<25:39, 27.01s/it]\u001b[A\n",
      "Loss=1.6388643980026245 Batch_id=41 Accuracy=26.12:  42%|████▏     | 41/98 [18:54<25:39, 27.01s/it]\u001b[A\n",
      "Loss=1.6388643980026245 Batch_id=41 Accuracy=26.12:  43%|████▎     | 42/98 [18:54<25:10, 26.97s/it]\u001b[A\n",
      "Loss=1.654446005821228 Batch_id=42 Accuracy=26.32:  43%|████▎     | 42/98 [19:18<25:10, 26.97s/it] \u001b[A\n",
      "Loss=1.654446005821228 Batch_id=42 Accuracy=26.32:  44%|████▍     | 43/98 [19:18<23:59, 26.18s/it]\u001b[A\n",
      "Loss=1.6709977388381958 Batch_id=43 Accuracy=26.58:  44%|████▍     | 43/98 [19:42<23:59, 26.18s/it]\u001b[A\n",
      "Loss=1.6709977388381958 Batch_id=43 Accuracy=26.58:  45%|████▍     | 44/98 [19:42<22:58, 25.53s/it]\u001b[A\n",
      "Loss=1.6901233196258545 Batch_id=44 Accuracy=26.86:  45%|████▍     | 44/98 [20:06<22:58, 25.53s/it]\u001b[A\n",
      "Loss=1.6901233196258545 Batch_id=44 Accuracy=26.86:  46%|████▌     | 45/98 [20:06<22:10, 25.11s/it]\u001b[A\n",
      "Loss=1.678951621055603 Batch_id=45 Accuracy=27.10:  46%|████▌     | 45/98 [20:31<22:10, 25.11s/it] \u001b[A\n",
      "Loss=1.678951621055603 Batch_id=45 Accuracy=27.10:  47%|████▋     | 46/98 [20:31<21:43, 25.07s/it]\u001b[A\n",
      "Loss=1.666917324066162 Batch_id=46 Accuracy=27.39:  47%|████▋     | 46/98 [20:56<21:43, 25.07s/it]\u001b[A\n",
      "Loss=1.666917324066162 Batch_id=46 Accuracy=27.39:  48%|████▊     | 47/98 [20:56<21:17, 25.05s/it]\u001b[A\n",
      "Loss=1.7079051733016968 Batch_id=47 Accuracy=27.61:  48%|████▊     | 47/98 [21:22<21:17, 25.05s/it]\u001b[A\n",
      "Loss=1.7079051733016968 Batch_id=47 Accuracy=27.61:  49%|████▉     | 48/98 [21:22<21:07, 25.34s/it]\u001b[A\n",
      "Loss=1.6896953582763672 Batch_id=48 Accuracy=27.81:  49%|████▉     | 48/98 [21:48<21:07, 25.34s/it]\u001b[A\n",
      "Loss=1.6896953582763672 Batch_id=48 Accuracy=27.81:  50%|█████     | 49/98 [21:48<20:40, 25.31s/it]\u001b[A\n",
      "Loss=1.6305639743804932 Batch_id=49 Accuracy=28.04:  50%|█████     | 49/98 [22:12<20:40, 25.31s/it]\u001b[A\n",
      "Loss=1.6305639743804932 Batch_id=49 Accuracy=28.04:  51%|█████     | 50/98 [22:12<20:05, 25.11s/it]\u001b[A\n",
      "Loss=1.6849523782730103 Batch_id=50 Accuracy=28.17:  51%|█████     | 50/98 [22:36<20:05, 25.11s/it]\u001b[A\n",
      "Loss=1.6849523782730103 Batch_id=50 Accuracy=28.17:  52%|█████▏    | 51/98 [22:36<19:22, 24.73s/it]\u001b[A\n",
      "Loss=1.659213900566101 Batch_id=51 Accuracy=28.37:  52%|█████▏    | 51/98 [23:01<19:22, 24.73s/it] \u001b[A\n",
      "Loss=1.659213900566101 Batch_id=51 Accuracy=28.37:  53%|█████▎    | 52/98 [23:01<18:59, 24.77s/it]\u001b[A\n",
      "Loss=1.6596461534500122 Batch_id=52 Accuracy=28.53:  53%|█████▎    | 52/98 [23:25<18:59, 24.77s/it]\u001b[A\n",
      "Loss=1.6596461534500122 Batch_id=52 Accuracy=28.53:  54%|█████▍    | 53/98 [23:25<18:27, 24.62s/it]\u001b[A\n",
      "Loss=1.6275887489318848 Batch_id=53 Accuracy=28.78:  54%|█████▍    | 53/98 [23:50<18:27, 24.62s/it]\u001b[A\n",
      "Loss=1.6275887489318848 Batch_id=53 Accuracy=28.78:  55%|█████▌    | 54/98 [23:50<17:59, 24.53s/it]\u001b[A\n",
      "Loss=1.56740140914917 Batch_id=54 Accuracy=28.99:  55%|█████▌    | 54/98 [24:14<17:59, 24.53s/it]  \u001b[A\n",
      "Loss=1.56740140914917 Batch_id=54 Accuracy=28.99:  56%|█████▌    | 55/98 [24:14<17:38, 24.62s/it]\u001b[A\n",
      "Loss=1.689499020576477 Batch_id=55 Accuracy=29.13:  56%|█████▌    | 55/98 [24:38<17:38, 24.62s/it]\u001b[A\n",
      "Loss=1.689499020576477 Batch_id=55 Accuracy=29.13:  57%|█████▋    | 56/98 [24:38<17:06, 24.44s/it]\u001b[A\n",
      "Loss=1.6330983638763428 Batch_id=56 Accuracy=29.31:  57%|█████▋    | 56/98 [25:06<17:06, 24.44s/it]\u001b[A\n",
      "Loss=1.6330983638763428 Batch_id=56 Accuracy=29.31:  58%|█████▊    | 57/98 [25:06<17:21, 25.41s/it]\u001b[A\n",
      "Loss=1.5930962562561035 Batch_id=57 Accuracy=29.53:  58%|█████▊    | 57/98 [25:35<17:21, 25.41s/it]\u001b[A\n",
      "Loss=1.5930962562561035 Batch_id=57 Accuracy=29.53:  59%|█████▉    | 58/98 [25:35<17:33, 26.35s/it]\u001b[A\n",
      "Loss=1.5728328227996826 Batch_id=58 Accuracy=29.72:  59%|█████▉    | 58/98 [26:19<17:33, 26.35s/it]\u001b[A\n",
      "Loss=1.5728328227996826 Batch_id=58 Accuracy=29.72:  60%|██████    | 59/98 [26:19<20:42, 31.85s/it]\u001b[A\n",
      "Loss=1.6163288354873657 Batch_id=59 Accuracy=29.86:  60%|██████    | 59/98 [26:52<20:42, 31.85s/it]\u001b[A\n",
      "Loss=1.6163288354873657 Batch_id=59 Accuracy=29.86:  61%|██████    | 60/98 [26:52<20:13, 31.94s/it]\u001b[A\n",
      "Loss=1.615359902381897 Batch_id=60 Accuracy=30.03:  61%|██████    | 60/98 [27:18<20:13, 31.94s/it] \u001b[A\n",
      "Loss=1.615359902381897 Batch_id=60 Accuracy=30.03:  62%|██████▏   | 61/98 [27:18<18:46, 30.44s/it]\u001b[A\n",
      "Loss=1.6096616983413696 Batch_id=61 Accuracy=30.18:  62%|██████▏   | 61/98 [27:51<18:46, 30.44s/it]\u001b[A\n",
      "Loss=1.6096616983413696 Batch_id=61 Accuracy=30.18:  63%|██████▎   | 62/98 [27:51<18:37, 31.03s/it]\u001b[A\n",
      "Loss=1.641790747642517 Batch_id=62 Accuracy=30.29:  63%|██████▎   | 62/98 [28:26<18:37, 31.03s/it] \u001b[A\n",
      "Loss=1.641790747642517 Batch_id=62 Accuracy=30.29:  64%|██████▍   | 63/98 [28:26<18:47, 32.21s/it]\u001b[A\n",
      "Loss=1.618126630783081 Batch_id=63 Accuracy=30.43:  64%|██████▍   | 63/98 [28:53<18:47, 32.21s/it]\u001b[A\n",
      "Loss=1.618126630783081 Batch_id=63 Accuracy=30.43:  65%|██████▌   | 64/98 [28:53<17:21, 30.63s/it]\u001b[A\n",
      "Loss=1.5329207181930542 Batch_id=64 Accuracy=30.59:  65%|██████▌   | 64/98 [29:23<17:21, 30.63s/it]\u001b[A\n",
      "Loss=1.5329207181930542 Batch_id=64 Accuracy=30.59:  66%|██████▋   | 65/98 [29:23<16:44, 30.43s/it]\u001b[A\n",
      "Loss=1.5974329710006714 Batch_id=65 Accuracy=30.74:  66%|██████▋   | 65/98 [29:49<16:44, 30.43s/it]\u001b[A\n",
      "Loss=1.5974329710006714 Batch_id=65 Accuracy=30.74:  67%|██████▋   | 66/98 [29:49<15:35, 29.22s/it]\u001b[A\n",
      "Loss=1.5133764743804932 Batch_id=66 Accuracy=30.94:  67%|██████▋   | 66/98 [30:19<15:35, 29.22s/it]\u001b[A\n",
      "Loss=1.5133764743804932 Batch_id=66 Accuracy=30.94:  68%|██████▊   | 67/98 [30:19<15:07, 29.27s/it]\u001b[A\n",
      "Loss=1.5495091676712036 Batch_id=67 Accuracy=31.14:  68%|██████▊   | 67/98 [30:45<15:07, 29.27s/it]\u001b[A\n",
      "Loss=1.5495091676712036 Batch_id=67 Accuracy=31.14:  69%|██████▉   | 68/98 [30:45<14:13, 28.45s/it]\u001b[A\n",
      "Loss=1.5816727876663208 Batch_id=68 Accuracy=31.29:  69%|██████▉   | 68/98 [31:10<14:13, 28.45s/it]\u001b[A\n",
      "Loss=1.5816727876663208 Batch_id=68 Accuracy=31.29:  70%|███████   | 69/98 [31:10<13:13, 27.37s/it]\u001b[A\n",
      "Loss=1.5913383960723877 Batch_id=69 Accuracy=31.41:  70%|███████   | 69/98 [31:35<13:13, 27.37s/it]\u001b[A\n",
      "Loss=1.5913383960723877 Batch_id=69 Accuracy=31.41:  71%|███████▏  | 70/98 [31:35<12:25, 26.61s/it]\u001b[A\n",
      "Loss=1.6082884073257446 Batch_id=70 Accuracy=31.53:  71%|███████▏  | 70/98 [31:59<12:25, 26.61s/it]\u001b[A\n",
      "Loss=1.6082884073257446 Batch_id=70 Accuracy=31.53:  72%|███████▏  | 71/98 [31:59<11:39, 25.89s/it]\u001b[A\n",
      "Loss=1.5223506689071655 Batch_id=71 Accuracy=31.70:  72%|███████▏  | 71/98 [32:26<11:39, 25.89s/it]\u001b[A\n",
      "Loss=1.5223506689071655 Batch_id=71 Accuracy=31.70:  73%|███████▎  | 72/98 [32:26<11:22, 26.23s/it]\u001b[A\n",
      "Loss=1.5144728422164917 Batch_id=72 Accuracy=31.88:  73%|███████▎  | 72/98 [32:51<11:22, 26.23s/it]\u001b[A\n",
      "Loss=1.5144728422164917 Batch_id=72 Accuracy=31.88:  74%|███████▍  | 73/98 [32:51<10:43, 25.75s/it]\u001b[A\n",
      "Loss=1.5014257431030273 Batch_id=73 Accuracy=32.04:  74%|███████▍  | 73/98 [33:16<10:43, 25.75s/it]\u001b[A\n",
      "Loss=1.5014257431030273 Batch_id=73 Accuracy=32.04:  76%|███████▌  | 74/98 [33:16<10:12, 25.53s/it]\u001b[A\n",
      "Loss=1.5825737714767456 Batch_id=74 Accuracy=32.14:  76%|███████▌  | 74/98 [33:40<10:12, 25.53s/it]\u001b[A\n",
      "Loss=1.5825737714767456 Batch_id=74 Accuracy=32.14:  77%|███████▋  | 75/98 [33:40<09:40, 25.26s/it]\u001b[A\n",
      "Loss=1.4973211288452148 Batch_id=75 Accuracy=32.29:  77%|███████▋  | 75/98 [34:05<09:40, 25.26s/it]\u001b[A\n",
      "Loss=1.4973211288452148 Batch_id=75 Accuracy=32.29:  78%|███████▊  | 76/98 [34:05<09:13, 25.15s/it]\u001b[A\n",
      "Loss=1.4137238264083862 Batch_id=76 Accuracy=32.51:  78%|███████▊  | 76/98 [34:31<09:13, 25.15s/it]\u001b[A\n",
      "Loss=1.4137238264083862 Batch_id=76 Accuracy=32.51:  79%|███████▊  | 77/98 [34:31<08:50, 25.24s/it]\u001b[A\n",
      "Loss=1.4930858612060547 Batch_id=77 Accuracy=32.68:  79%|███████▊  | 77/98 [34:59<08:50, 25.24s/it]\u001b[A\n",
      "Loss=1.4930858612060547 Batch_id=77 Accuracy=32.68:  80%|███████▉  | 78/98 [34:59<08:45, 26.29s/it]\u001b[A\n",
      "Loss=1.5229135751724243 Batch_id=78 Accuracy=32.82:  80%|███████▉  | 78/98 [35:25<08:45, 26.29s/it]\u001b[A\n",
      "Loss=1.5229135751724243 Batch_id=78 Accuracy=32.82:  81%|████████  | 79/98 [35:25<08:18, 26.21s/it]\u001b[A\n",
      "Loss=1.4667226076126099 Batch_id=79 Accuracy=33.00:  81%|████████  | 79/98 [35:51<08:18, 26.21s/it]\u001b[A\n",
      "Loss=1.4667226076126099 Batch_id=79 Accuracy=33.00:  82%|████████▏ | 80/98 [35:51<07:48, 26.01s/it]\u001b[A\n",
      "Loss=1.4986985921859741 Batch_id=80 Accuracy=33.15:  82%|████████▏ | 80/98 [36:17<07:48, 26.01s/it]\u001b[A\n",
      "Loss=1.4986985921859741 Batch_id=80 Accuracy=33.15:  83%|████████▎ | 81/98 [36:17<07:21, 25.98s/it]\u001b[A\n",
      "Loss=1.4798808097839355 Batch_id=81 Accuracy=33.31:  83%|████████▎ | 81/98 [36:42<07:21, 25.98s/it]\u001b[A\n",
      "Loss=1.4798808097839355 Batch_id=81 Accuracy=33.31:  84%|████████▎ | 82/98 [36:42<06:52, 25.78s/it]\u001b[A\n",
      "Loss=1.4261410236358643 Batch_id=82 Accuracy=33.50:  84%|████████▎ | 82/98 [37:09<06:52, 25.78s/it]\u001b[A\n",
      "Loss=1.4261410236358643 Batch_id=82 Accuracy=33.50:  85%|████████▍ | 83/98 [37:09<06:31, 26.07s/it]\u001b[A\n",
      "Loss=1.4543648958206177 Batch_id=83 Accuracy=33.67:  85%|████████▍ | 83/98 [37:44<06:31, 26.07s/it]\u001b[A\n",
      "Loss=1.4543648958206177 Batch_id=83 Accuracy=33.67:  86%|████████▌ | 84/98 [37:44<06:43, 28.83s/it]\u001b[A\n",
      "Loss=1.5179027318954468 Batch_id=84 Accuracy=33.78:  86%|████████▌ | 84/98 [38:20<06:43, 28.83s/it]\u001b[A\n",
      "Loss=1.5179027318954468 Batch_id=84 Accuracy=33.78:  87%|████████▋ | 85/98 [38:20<06:43, 31.00s/it]\u001b[A\n",
      "Loss=1.4615072011947632 Batch_id=85 Accuracy=33.94:  87%|████████▋ | 85/98 [38:49<06:43, 31.00s/it]\u001b[A\n",
      "Loss=1.4615072011947632 Batch_id=85 Accuracy=33.94:  88%|████████▊ | 86/98 [38:49<06:02, 30.21s/it]\u001b[A\n",
      "Loss=1.4583040475845337 Batch_id=86 Accuracy=34.11:  88%|████████▊ | 86/98 [39:16<06:02, 30.21s/it]\u001b[A\n",
      "Loss=1.4583040475845337 Batch_id=86 Accuracy=34.11:  89%|████████▉ | 87/98 [39:16<05:23, 29.40s/it]\u001b[A\n",
      "Loss=1.5128809213638306 Batch_id=87 Accuracy=34.26:  89%|████████▉ | 87/98 [39:43<05:23, 29.40s/it]\u001b[A\n",
      "Loss=1.5128809213638306 Batch_id=87 Accuracy=34.26:  90%|████████▉ | 88/98 [39:43<04:47, 28.75s/it]\u001b[A\n",
      "Loss=1.4993705749511719 Batch_id=88 Accuracy=34.37:  90%|████████▉ | 88/98 [40:11<04:47, 28.75s/it]\u001b[A\n",
      "Loss=1.4993705749511719 Batch_id=88 Accuracy=34.37:  91%|█████████ | 89/98 [40:11<04:14, 28.30s/it]\u001b[A\n",
      "Loss=1.4810292720794678 Batch_id=89 Accuracy=34.49:  91%|█████████ | 89/98 [40:38<04:14, 28.30s/it]\u001b[A\n",
      "Loss=1.4810292720794678 Batch_id=89 Accuracy=34.49:  92%|█████████▏| 90/98 [40:38<03:43, 27.88s/it]\u001b[A\n",
      "Loss=1.443839192390442 Batch_id=90 Accuracy=34.63:  92%|█████████▏| 90/98 [41:05<03:43, 27.88s/it] \u001b[A\n",
      "Loss=1.443839192390442 Batch_id=90 Accuracy=34.63:  93%|█████████▎| 91/98 [41:05<03:13, 27.68s/it]\u001b[A\n",
      "Loss=1.4989577531814575 Batch_id=91 Accuracy=34.72:  93%|█████████▎| 91/98 [41:32<03:13, 27.68s/it]\u001b[A\n",
      "Loss=1.4989577531814575 Batch_id=91 Accuracy=34.72:  94%|█████████▍| 92/98 [41:32<02:45, 27.53s/it]\u001b[A\n",
      "Loss=1.444715142250061 Batch_id=92 Accuracy=34.81:  94%|█████████▍| 92/98 [42:00<02:45, 27.53s/it] \u001b[A\n",
      "Loss=1.444715142250061 Batch_id=92 Accuracy=34.81:  95%|█████████▍| 93/98 [42:00<02:18, 27.65s/it]\u001b[A\n",
      "Loss=1.48440420627594 Batch_id=93 Accuracy=34.94:  95%|█████████▍| 93/98 [42:27<02:18, 27.65s/it] \u001b[A\n",
      "Loss=1.48440420627594 Batch_id=93 Accuracy=34.94:  96%|█████████▌| 94/98 [42:27<01:49, 27.37s/it]\u001b[A\n",
      "Loss=1.4616990089416504 Batch_id=94 Accuracy=35.09:  96%|█████████▌| 94/98 [42:54<01:49, 27.37s/it]\u001b[A\n",
      "Loss=1.4616990089416504 Batch_id=94 Accuracy=35.09:  97%|█████████▋| 95/98 [42:54<01:21, 27.26s/it]\u001b[A\n",
      "Loss=1.3858401775360107 Batch_id=95 Accuracy=35.24:  97%|█████████▋| 95/98 [43:21<01:21, 27.26s/it]\u001b[A\n",
      "Loss=1.3858401775360107 Batch_id=95 Accuracy=35.24:  98%|█████████▊| 96/98 [43:21<00:54, 27.36s/it]\u001b[A\n",
      "Loss=1.4577668905258179 Batch_id=96 Accuracy=35.36:  98%|█████████▊| 96/98 [43:50<00:54, 27.36s/it]\u001b[A\n",
      "Loss=1.4577668905258179 Batch_id=96 Accuracy=35.36:  99%|█████████▉| 97/98 [43:50<00:27, 27.84s/it]\u001b[A\n",
      "Loss=1.4391411542892456 Batch_id=97 Accuracy=35.43:  99%|█████████▉| 97/98 [44:08<00:27, 27.84s/it]\u001b[A\n",
      "Loss=1.4391411542892456 Batch_id=97 Accuracy=35.43: 100%|██████████| 98/98 [44:08<00:00, 27.02s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/98 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.3976, Accuracy: 4953/10000 (49.53%)\n",
      "\n",
      "EPOCHS : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss=1.3629841804504395 Batch_id=0 Accuracy=52.73:   0%|          | 0/98 [00:42<?, ?it/s]\u001b[A\n",
      "Loss=1.3629841804504395 Batch_id=0 Accuracy=52.73:   1%|          | 1/98 [00:42<1:08:12, 42.19s/it]\u001b[A\n",
      "Loss=1.4689592123031616 Batch_id=1 Accuracy=49.32:   1%|          | 1/98 [01:14<1:08:12, 42.19s/it]\u001b[A\n",
      "Loss=1.4689592123031616 Batch_id=1 Accuracy=49.32:   2%|▏         | 2/98 [01:14<1:02:41, 39.19s/it]\u001b[A\n",
      "Loss=1.4366075992584229 Batch_id=2 Accuracy=48.18:   2%|▏         | 2/98 [01:45<1:02:41, 39.19s/it]\u001b[A\n",
      "Loss=1.4366075992584229 Batch_id=2 Accuracy=48.18:   3%|▎         | 3/98 [01:45<58:07, 36.71s/it]  \u001b[A\n",
      "Loss=1.449281096458435 Batch_id=3 Accuracy=48.00:   3%|▎         | 3/98 [02:23<58:07, 36.71s/it] \u001b[A\n",
      "Loss=1.449281096458435 Batch_id=3 Accuracy=48.00:   4%|▍         | 4/98 [02:23<58:02, 37.05s/it]\u001b[A\n",
      "Loss=1.4567406177520752 Batch_id=4 Accuracy=47.66:   4%|▍         | 4/98 [02:55<58:02, 37.05s/it]\u001b[A\n",
      "Loss=1.4567406177520752 Batch_id=4 Accuracy=47.66:   5%|▌         | 5/98 [02:55<55:06, 35.56s/it]\u001b[A\n",
      "Loss=1.4073094129562378 Batch_id=5 Accuracy=48.01:   5%|▌         | 5/98 [03:26<55:06, 35.56s/it]\u001b[A\n",
      "Loss=1.4073094129562378 Batch_id=5 Accuracy=48.01:   6%|▌         | 6/98 [03:26<52:34, 34.28s/it]\u001b[A\n",
      "Loss=1.425309658050537 Batch_id=6 Accuracy=48.19:   6%|▌         | 6/98 [03:54<52:34, 34.28s/it] \u001b[A\n",
      "Loss=1.425309658050537 Batch_id=6 Accuracy=48.19:   7%|▋         | 7/98 [03:54<49:07, 32.39s/it]\u001b[A\n",
      "Loss=1.455370306968689 Batch_id=7 Accuracy=47.73:   7%|▋         | 7/98 [04:24<49:07, 32.39s/it]\u001b[A\n",
      "Loss=1.455370306968689 Batch_id=7 Accuracy=47.73:   8%|▊         | 8/98 [04:24<47:28, 31.65s/it]\u001b[A\n",
      "Loss=1.4356719255447388 Batch_id=8 Accuracy=47.72:   8%|▊         | 8/98 [04:51<47:28, 31.65s/it]\u001b[A\n",
      "Loss=1.4356719255447388 Batch_id=8 Accuracy=47.72:   9%|▉         | 9/98 [04:51<45:00, 30.34s/it]\u001b[A\n",
      "Loss=1.474873423576355 Batch_id=9 Accuracy=47.73:   9%|▉         | 9/98 [05:22<45:00, 30.34s/it] \u001b[A\n",
      "Loss=1.474873423576355 Batch_id=9 Accuracy=47.73:  10%|█         | 10/98 [05:22<44:32, 30.37s/it]\u001b[A\n",
      "Loss=1.4209240674972534 Batch_id=10 Accuracy=47.69:  10%|█         | 10/98 [05:54<44:32, 30.37s/it]\u001b[A\n",
      "Loss=1.4209240674972534 Batch_id=10 Accuracy=47.69:  11%|█         | 11/98 [05:54<44:55, 30.98s/it]\u001b[A\n",
      "Loss=1.3520582914352417 Batch_id=11 Accuracy=47.93:  11%|█         | 11/98 [06:22<44:55, 30.98s/it]\u001b[A\n",
      "Loss=1.3520582914352417 Batch_id=11 Accuracy=47.93:  12%|█▏        | 12/98 [06:22<43:16, 30.19s/it]\u001b[A\n",
      "Loss=1.4357222318649292 Batch_id=12 Accuracy=47.96:  12%|█▏        | 12/98 [06:51<43:16, 30.19s/it]\u001b[A\n",
      "Loss=1.4357222318649292 Batch_id=12 Accuracy=47.96:  13%|█▎        | 13/98 [06:51<41:57, 29.61s/it]\u001b[A\n",
      "Loss=1.4118788242340088 Batch_id=13 Accuracy=48.17:  13%|█▎        | 13/98 [07:19<41:57, 29.61s/it]\u001b[A\n",
      "Loss=1.4118788242340088 Batch_id=13 Accuracy=48.17:  14%|█▍        | 14/98 [07:19<40:43, 29.08s/it]\u001b[A\n",
      "Loss=1.404935598373413 Batch_id=14 Accuracy=48.19:  14%|█▍        | 14/98 [07:45<40:43, 29.08s/it] \u001b[A\n",
      "Loss=1.404935598373413 Batch_id=14 Accuracy=48.19:  15%|█▌        | 15/98 [07:45<39:18, 28.42s/it]\u001b[A\n",
      "Loss=1.4565893411636353 Batch_id=15 Accuracy=47.97:  15%|█▌        | 15/98 [08:15<39:18, 28.42s/it]\u001b[A\n",
      "Loss=1.4565893411636353 Batch_id=15 Accuracy=47.97:  16%|█▋        | 16/98 [08:15<39:15, 28.73s/it]\u001b[A\n",
      "Loss=1.385694980621338 Batch_id=16 Accuracy=48.06:  16%|█▋        | 16/98 [08:41<39:15, 28.73s/it] \u001b[A\n",
      "Loss=1.385694980621338 Batch_id=16 Accuracy=48.06:  17%|█▋        | 17/98 [08:41<37:49, 28.01s/it]\u001b[A\n",
      "Loss=1.4056057929992676 Batch_id=17 Accuracy=48.10:  17%|█▋        | 17/98 [09:09<37:49, 28.01s/it]\u001b[A\n",
      "Loss=1.4056057929992676 Batch_id=17 Accuracy=48.10:  18%|█▊        | 18/98 [09:09<37:08, 27.86s/it]\u001b[A\n",
      "Loss=1.335320234298706 Batch_id=18 Accuracy=48.09:  18%|█▊        | 18/98 [09:36<37:08, 27.86s/it] \u001b[A\n",
      "Loss=1.335320234298706 Batch_id=18 Accuracy=48.09:  19%|█▉        | 19/98 [09:36<36:21, 27.61s/it]\u001b[A\n",
      "Loss=1.396421194076538 Batch_id=19 Accuracy=48.07:  19%|█▉        | 19/98 [10:03<36:21, 27.61s/it]\u001b[A\n",
      "Loss=1.396421194076538 Batch_id=19 Accuracy=48.07:  20%|██        | 20/98 [10:03<35:47, 27.54s/it]\u001b[A\n",
      "Loss=1.2834458351135254 Batch_id=20 Accuracy=48.27:  20%|██        | 20/98 [10:30<35:47, 27.54s/it]\u001b[A\n",
      "Loss=1.2834458351135254 Batch_id=20 Accuracy=48.27:  21%|██▏       | 21/98 [10:30<34:59, 27.26s/it]\u001b[A\n",
      "Loss=1.4123079776763916 Batch_id=21 Accuracy=48.36:  21%|██▏       | 21/98 [10:58<34:59, 27.26s/it]\u001b[A\n",
      "Loss=1.4123079776763916 Batch_id=21 Accuracy=48.36:  22%|██▏       | 22/98 [10:58<35:01, 27.65s/it]\u001b[A\n",
      "Loss=1.3470866680145264 Batch_id=22 Accuracy=48.49:  22%|██▏       | 22/98 [11:26<35:01, 27.65s/it]\u001b[A\n",
      "Loss=1.3470866680145264 Batch_id=22 Accuracy=48.49:  23%|██▎       | 23/98 [11:26<34:27, 27.57s/it]\u001b[A\n",
      "Loss=1.3312196731567383 Batch_id=23 Accuracy=48.62:  23%|██▎       | 23/98 [11:53<34:27, 27.57s/it]\u001b[A\n",
      "Loss=1.3312196731567383 Batch_id=23 Accuracy=48.62:  24%|██▍       | 24/98 [11:53<33:53, 27.49s/it]\u001b[A\n",
      "Loss=1.4114313125610352 Batch_id=24 Accuracy=48.60:  24%|██▍       | 24/98 [12:22<33:53, 27.49s/it]\u001b[A\n",
      "Loss=1.4114313125610352 Batch_id=24 Accuracy=48.60:  26%|██▌       | 25/98 [12:22<34:03, 27.99s/it]\u001b[A\n",
      "Loss=1.3574122190475464 Batch_id=25 Accuracy=48.66:  26%|██▌       | 25/98 [12:57<34:03, 27.99s/it]\u001b[A\n",
      "Loss=1.3574122190475464 Batch_id=25 Accuracy=48.66:  27%|██▋       | 26/98 [12:57<36:02, 30.04s/it]\u001b[A\n",
      "Loss=1.3235602378845215 Batch_id=26 Accuracy=48.71:  27%|██▋       | 26/98 [13:38<36:02, 30.04s/it]\u001b[A\n",
      "Loss=1.3235602378845215 Batch_id=26 Accuracy=48.71:  28%|██▊       | 27/98 [13:38<39:20, 33.24s/it]\u001b[A\n",
      "Loss=1.3666133880615234 Batch_id=27 Accuracy=48.80:  28%|██▊       | 27/98 [14:06<39:20, 33.24s/it]\u001b[A\n",
      "Loss=1.3666133880615234 Batch_id=27 Accuracy=48.80:  29%|██▊       | 28/98 [14:06<37:08, 31.84s/it]\u001b[A\n",
      "Loss=1.4520008563995361 Batch_id=28 Accuracy=48.74:  29%|██▊       | 28/98 [14:35<37:08, 31.84s/it]\u001b[A\n",
      "Loss=1.4520008563995361 Batch_id=28 Accuracy=48.74:  30%|██▉       | 29/98 [14:35<35:31, 30.89s/it]\u001b[A\n",
      "Loss=1.37619149684906 Batch_id=29 Accuracy=48.76:  30%|██▉       | 29/98 [15:04<35:31, 30.89s/it]  \u001b[A\n",
      "Loss=1.37619149684906 Batch_id=29 Accuracy=48.76:  31%|███       | 30/98 [15:04<34:15, 30.22s/it]\u001b[A\n",
      "Loss=1.3470450639724731 Batch_id=30 Accuracy=48.81:  31%|███       | 30/98 [15:32<34:15, 30.22s/it]\u001b[A\n",
      "Loss=1.3470450639724731 Batch_id=30 Accuracy=48.81:  32%|███▏      | 31/98 [15:32<33:08, 29.69s/it]\u001b[A\n",
      "Loss=1.400977611541748 Batch_id=31 Accuracy=48.76:  32%|███▏      | 31/98 [16:02<33:08, 29.69s/it] \u001b[A\n",
      "Loss=1.400977611541748 Batch_id=31 Accuracy=48.76:  33%|███▎      | 32/98 [16:02<32:42, 29.74s/it]\u001b[A\n",
      "Loss=1.2693551778793335 Batch_id=32 Accuracy=48.92:  33%|███▎      | 32/98 [16:36<32:42, 29.74s/it]\u001b[A\n",
      "Loss=1.2693551778793335 Batch_id=32 Accuracy=48.92:  34%|███▎      | 33/98 [16:36<33:34, 30.99s/it]\u001b[A\n",
      "Loss=1.3683022260665894 Batch_id=33 Accuracy=48.93:  34%|███▎      | 33/98 [17:04<33:34, 30.99s/it]\u001b[A\n",
      "Loss=1.3683022260665894 Batch_id=33 Accuracy=48.93:  35%|███▍      | 34/98 [17:04<32:15, 30.24s/it]\u001b[A\n",
      "Loss=1.393538236618042 Batch_id=34 Accuracy=48.99:  35%|███▍      | 34/98 [17:30<32:15, 30.24s/it] \u001b[A\n",
      "Loss=1.393538236618042 Batch_id=34 Accuracy=48.99:  36%|███▌      | 35/98 [17:30<30:20, 28.89s/it]\u001b[A\n",
      "Loss=1.2190346717834473 Batch_id=35 Accuracy=49.26:  36%|███▌      | 35/98 [17:55<30:20, 28.89s/it]\u001b[A\n",
      "Loss=1.2190346717834473 Batch_id=35 Accuracy=49.26:  37%|███▋      | 36/98 [17:55<28:47, 27.86s/it]\u001b[A\n",
      "Loss=1.3519665002822876 Batch_id=36 Accuracy=49.25:  37%|███▋      | 36/98 [18:22<28:47, 27.86s/it]\u001b[A\n",
      "Loss=1.3519665002822876 Batch_id=36 Accuracy=49.25:  38%|███▊      | 37/98 [18:22<28:04, 27.61s/it]\u001b[A\n",
      "Loss=1.3519147634506226 Batch_id=37 Accuracy=49.29:  38%|███▊      | 37/98 [18:52<28:04, 27.61s/it]\u001b[A\n",
      "Loss=1.3519147634506226 Batch_id=37 Accuracy=49.29:  39%|███▉      | 38/98 [18:52<28:17, 28.30s/it]\u001b[A\n",
      "Loss=1.4102296829223633 Batch_id=38 Accuracy=49.35:  39%|███▉      | 38/98 [19:21<28:17, 28.30s/it]\u001b[A\n",
      "Loss=1.4102296829223633 Batch_id=38 Accuracy=49.35:  40%|███▉      | 39/98 [19:21<27:48, 28.28s/it]\u001b[A\n",
      "Loss=1.4381906986236572 Batch_id=39 Accuracy=49.28:  40%|███▉      | 39/98 [19:49<27:48, 28.28s/it]\u001b[A\n",
      "Loss=1.4381906986236572 Batch_id=39 Accuracy=49.28:  41%|████      | 40/98 [19:49<27:30, 28.46s/it]\u001b[A\n",
      "Loss=1.4131622314453125 Batch_id=40 Accuracy=49.29:  41%|████      | 40/98 [20:25<27:30, 28.46s/it]\u001b[A\n",
      "Loss=1.4131622314453125 Batch_id=40 Accuracy=49.29:  42%|████▏     | 41/98 [20:25<28:56, 30.47s/it]\u001b[A\n",
      "Loss=1.3168617486953735 Batch_id=41 Accuracy=49.42:  42%|████▏     | 41/98 [21:03<28:56, 30.47s/it]\u001b[A\n",
      "Loss=1.3168617486953735 Batch_id=41 Accuracy=49.42:  43%|████▎     | 42/98 [21:03<30:41, 32.88s/it]\u001b[A\n",
      "Loss=1.3028076887130737 Batch_id=42 Accuracy=49.46:  43%|████▎     | 42/98 [21:39<30:41, 32.88s/it]\u001b[A\n",
      "Loss=1.3028076887130737 Batch_id=42 Accuracy=49.46:  44%|████▍     | 43/98 [21:41<27:45, 30.27s/it]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-131de9e51a86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'EPOCHS : {i}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmisclassified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-e79189827e3e>\u001b[0m in \u001b[0;36mmodel_training\u001b[0;34m(model, device, train_dataloader, optimizer, train_acc, train_losses)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-70f21a6dcf56>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-70f21a6dcf56>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.05, patience=2, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)\n",
    "\n",
    "train_acc = []\n",
    "train_losses = []\n",
    "test_acc = []\n",
    "test_losses = []\n",
    "\n",
    "EPOCHS = 40\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(f'EPOCHS : {i}')\n",
    "    model_training(model, device, trainloader, optimizer, train_acc, train_losses)\n",
    "    scheduler.step(train_losses[-1])\n",
    "    misclassified = model_testing(model, device, testloader, test_acc, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
